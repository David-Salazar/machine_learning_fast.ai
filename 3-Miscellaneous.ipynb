{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Broadcastings\n",
    "\n",
    "> The term broadcasting describes how numpy treats arrays with \n",
    "different shapes during arithmetic operations. Subject to certain \n",
    "constraints, the smaller array is “broadcast” across the larger \n",
    "array so that they have compatible shapes. Broadcasting provides a \n",
    "means of vectorizing array operations so that looping occurs in C\n",
    "instead of Python. It does this without making needless copies of \n",
    "data and usually leads to efficient algorithm implementations.\n",
    "\n",
    "When operating on two arrays, Numpy/PyTorch compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when:\n",
    "\n",
    "- they are equal, or\n",
    "- one of them is 1.\n",
    "- they only disagree on exceeding innermost dimensions. i.e., dimensions that do not exist on smaller object. \n",
    "\n",
    "After it has checked that (and passed, otherwise broadcasting cannot continue), it will broadcast the innermost elements of the broadcasted elements into the larger shape. i.e., a scalar  (1, 1, 1) into a 2x3x3 tensor: repeat the scalar twice. For each of instantiation of the scalar, broadcast into a matrix of 3x3. Then you have a new tensor of 2x3x3: 2 matrices of scalar.\n",
    "\n",
    "If you have a 1-d tensor (3, 1, 1) and broadcast it into (3, 3, 3): each of the three elements of the \n",
    "original 1-d tensor get broadcasted into 3x3 matrices. Thus, you end up with a 3-d tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "m = np.array([[1, 2, 3], [4,5,6], [7,8,9]])\n",
    "print(m); m.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  6],\n",
       "       [ 8, 10, 12],\n",
       "       [14, 16, 18]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trailing dimension are compatible (one of them is of size 1, such that the broadcasting is trivial) but the first dimension is not compatible. So that the broadcasted term 1-d 3 component vector is then broadcasted across the first dimension. Then, the matrices are multiplicated element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30]\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "c = np.array([10,20,30]); \n",
    "print(c); print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding vector to a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 22, 33],\n",
       "       [14, 25, 36],\n",
       "       [17, 28, 39]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the trailing dimension is not comaptible (empty vs 3). Then, the initial tensor is repeated across this missing dimension to make them compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30]\n",
      "\n",
      " [[10 20 30]\n",
      " [10 20 30]\n",
      " [10 20 30]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[11, 22, 33],\n",
       "       [14, 25, 36],\n",
       "       [17, 28, 39]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(c); print(\"\\n\", np.broadcast_to(c, m.shape)); np.broadcast_to(c, m.shape) + m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column vector to a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:, None].shape # i.e., recast such that we can have another dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10],\n",
       "       [20],\n",
       "       [30]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add this new vector, the trailing dimension will be compatible and this vector will be broadcasted into the new shape. i.e., we have a 1-d tensor with 3 components and need a 2-d 3x3. That is, we have the 0x3, thus create 3 of the columns we have and broadcast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 10, 10],\n",
       "       [20, 20, 20],\n",
       "       [30, 30, 30]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to(c[:, None], m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 12, 13],\n",
       "       [24, 25, 26],\n",
       "       [37, 38, 39]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to(c[:, None], m.shape) + m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over channels of pixels\n",
    "\n",
    "Let's imagine we have an image with 3 channels (RGB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1],\n",
       "        [1, 1]],\n",
       "\n",
       "       [[2, 2],\n",
       "        [2, 2]],\n",
       "\n",
       "       [[3, 3],\n",
       "        [3, 3]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "image = np.broadcast_to(x[:, None, None], (3, 2, 2))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to divide the first channel by 4, the second by 6 and the third by 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.25      , 0.25      ],\n",
       "        [0.25      , 0.25      ]],\n",
       "\n",
       "       [[0.33333333, 0.33333333],\n",
       "        [0.33333333, 0.33333333]],\n",
       "\n",
       "       [[0.375     , 0.375     ],\n",
       "        [0.375     , 0.375     ]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dividers = np.array([4, 6, 8])\n",
    "image/dividers[:, None, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both tensors are compatible across each of the ranks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 1) (3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dividers[:, None, None].shape, image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, what happens is the first dimension gets broadcasted such that each of the elements (4, 6, 8) is compatible with a 2-d tensor of shape 2x2. Then, element division is applied to the 3-d tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2, 2)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([image, image]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.25      , 0.25      ],\n",
       "         [0.25      , 0.25      ]],\n",
       "\n",
       "        [[0.33333333, 0.33333333],\n",
       "         [0.33333333, 0.33333333]],\n",
       "\n",
       "        [[0.375     , 0.375     ],\n",
       "         [0.375     , 0.375     ]]],\n",
       "\n",
       "\n",
       "       [[[0.25      , 0.25      ],\n",
       "         [0.25      , 0.25      ]],\n",
       "\n",
       "        [[0.33333333, 0.33333333],\n",
       "         [0.33333333, 0.33333333]],\n",
       "\n",
       "        [[0.375     , 0.375     ],\n",
       "         [0.375     , 0.375     ]]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([image, image])/dividers[:, None, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can straightfowardly use broadcasting when the only thing they differ is the innermost dimension, as above. Just repeat the (d-1) tensor you have as many times as necessary. However, this only works with the innermost dimension. With outermost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,2,2) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-fa960843daeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mdividers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2,2) (3,) "
     ]
    }
   ],
   "source": [
    "image/ dividers # broadcasting cannot continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1- Broadcasting is very simple when all the outermost dimensions agree, but the first do not:\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 2, 2) (3, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.array([image, image]).shape, dividers[:, None, None].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.25      , 0.25      ],\n",
       "         [0.25      , 0.25      ]],\n",
       "\n",
       "        [[0.33333333, 0.33333333],\n",
       "         [0.33333333, 0.33333333]],\n",
       "\n",
       "        [[0.375     , 0.375     ],\n",
       "         [0.375     , 0.375     ]]],\n",
       "\n",
       "\n",
       "       [[[0.25      , 0.25      ],\n",
       "         [0.25      , 0.25      ]],\n",
       "\n",
       "        [[0.33333333, 0.33333333],\n",
       "         [0.33333333, 0.33333333]],\n",
       "\n",
       "        [[0.375     , 0.375     ],\n",
       "         [0.375     , 0.375     ]]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([image, image])/ dividers[:, None, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- If any of the outermost dimensions do not agree, broadcasting is impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,2,2) (3,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-7374e1e9ed93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdividers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2,2) (3,1) "
     ]
    }
   ],
   "source": [
    "image / dividers[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- After dimensions agree, broadcasting cannot be more simple:\n",
    "    Repeat each of the innermost figures, as many times as necessary to fill the other dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4, 4],\n",
       "        [4, 4]],\n",
       "\n",
       "       [[6, 6],\n",
       "        [6, 6]],\n",
       "\n",
       "       [[8, 8],\n",
       "        [8, 8]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to(dividers[:, None, None], image.shape) # innermost figure: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[4, 4],\n",
       "         [4, 4]],\n",
       "\n",
       "        [[6, 6],\n",
       "         [6, 6]],\n",
       "\n",
       "        [[8, 8],\n",
       "         [8, 8]]],\n",
       "\n",
       "\n",
       "       [[[4, 4],\n",
       "         [4, 4]],\n",
       "\n",
       "        [[6, 6],\n",
       "         [6, 6]],\n",
       "\n",
       "        [[8, 8],\n",
       "         [8, 8]]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to(dividers[:, None, None], np.array([image, image]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "I have read much, but the example that Jeremy used to really understand regularization was great: regularization is a prior around zero. You can change that prior by playing around with what gets passed to the loss function. \n",
    "\n",
    "For example, he used a combination of Naive Bayes and Logistic Regression. Naive Bayes is equivalent to multiply the bag of words (one-hot-encoded matrix) by a prior and then taking the product across these probabilities as if their intersection were null (hence Naive) (with log probs the product changes to a sum). That is, the matrix gets multiplicated by some weights (the prior) and then summed. Essentially, a linear model.\n",
    "\n",
    "What if we learned those weights from the data, a logistic regression. If we regularize, we say to the algorithm, our prior is that all those weights should be zero.\n",
    "\n",
    "What if instead, we learned the weights, add a constant (say 0.5) and multiply by the priors? Then, the effective weights that will be used to predict, that tend to be zero, will be ~the priors. Thus, if we implement l2 regularization, the prior (with weights around zero) will be use the priors from naive bayes. If you want to negate those priors, you will have to make the weights negative, but that will cost you in the loss function due to l2 measure on the weights. \n",
    "\n",
    "Thus, Jeremy implemented an algorithm that took into account prior information to the model and made the model put attention to it through the use of regularization. \n",
    "\n",
    "Very cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "> about embeddings like word embeddings word to their core glove or whatever and people love to make them sound like this amazing new complex neural net thing right they're not embedding means make a multiplication by a one hot encoded matrix faster by replacing it with a simple array. where possible, it is best to treat things as categorical variables. easier for a neural net to find a functional form that exploits the difference between values.\n",
    "\n",
    "Imagine a one hot-encoded matrix: i.e., columns of zeroes and ones. To multiply that matrix by a vector, is equivalent to insert zeroes on the vector where there are zeroes on the rows of the matrix and then sum their elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0, 1, 1], [1, 0, 0], [1, 1, 1]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  5, 10])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ np.array([5, 3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when you have a massive one-hot encoded matrix, to perform so many multiplications is inefficient. But there's a way to workaround: store your massive matrix as a sparse matrix: i.e., only store the position of the components that are not zero. Then, subset your vector with these positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([5, 3, 2])[[1, 2]].sum() # first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([5, 3, 2])[[0]].sum() # second row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([5, 3, 2])[[0, 1, 2]].sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we find a representation of the one-hot encoding matrix that multi-layered models can use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using embeddings on Neural Networks\n",
    "\n",
    "By doing this, we can skip our SGD models a lot more suffering than they should endure: we do not have to use ordinal variables, which makes it very difficult for them to find information gain, nor have to endure the computational burden from using a one-hot encoding matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

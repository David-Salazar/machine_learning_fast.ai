{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "## What are they?\n",
    "\n",
    "From the words of Jeremy Howard himself (from the YouTube transcript of the lectures):\n",
    "\n",
    "> in brief a random forest is a\n",
    "kind of universal machine learning\n",
    "technique it's a way of predicting\n",
    "something that can be of any kind it\n",
    "could be a category like is it a dog or\n",
    "a cat or it could be a continuous\n",
    "function you aspera like price it can\n",
    "predict it with columns of pretty much\n",
    "any kind pixel data zip codes revenues\n",
    "whatever in general it doesn't overfit\n",
    "it can and will learn to check whether\n",
    "it is but it doesn't generally overfit\n",
    "too badly and it's very very easy to\n",
    "make to stop it from overfitting you\n",
    "don't need and we'll talk more about\n",
    "this you don't need a separate\n",
    "validation set in general it can tell\n",
    "you how well it\n",
    "generalizes even if you only have one\n",
    "data set it has few if any statistical\n",
    "assumptions it doesn't assume that your\n",
    "data is normally distributed it doesn't\n",
    "assume that the relationship so linear\n",
    "it doesn't assume that you've just\n",
    "specified the interactions it requires\n",
    "very few pieces of feature engineering\n",
    "for many different types of situation\n",
    "you don't have to take the log of the\n",
    "data you don't have to multiply\n",
    "interactions together so in other words\n",
    "it's a great place to start right if\n",
    "your first random forest does very\n",
    "little useful then that's a sign that\n",
    "there might be problems with your data\n",
    "like it's designed to work pretty much\n",
    "first off >\n",
    "\n",
    "Summarizing, it's a universal machine learning technique. Why?\n",
    "\n",
    "1. It's a way of predicting either a category or regressor. \n",
    "1. It can handel any type of columns (either continuous or not). It can handle them as categorical variables (ordinal or not) or one-hot encoding. This last part is due to the fact that it can split multiple times and arrive at a split that is essentially a dummy for any of the categories. \n",
    "1. In general, it does not overfit that easily and has very few parameters to check if indeed it does overfit. \n",
    "1. You can use Out Of Bags samples as validation samples. \n",
    "1. It has few if any statistcial assumptions. i.e., it does not assume a linear relationship nor does assume which interactions to use. Thus, it requires very few pieces of feature engineering. For clarification of how big this is, compare it to a logistic regression: \n",
    "> it's pretty much always possible to\n",
    "create a simple like logistic regression\n",
    "which is as good as pretty much any\n",
    "random first if you know ahead of time\n",
    "exactly what variables you need exactly\n",
    "how they interact exactly how they need\n",
    "to be transformed and so forth right so >\n",
    "\n",
    "Compare this to the F. Chollet's remarks about the multi-stage nature of Neural Networks and the resulting data-based feature engineering. \n",
    "\n",
    "6. It scales very well as the number of observations grow. This is a result from the bootstraping used to decorrelate the weak learners: i.e., you can train each tree on a sample from the whole dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "\n",
    "A forest is a set of trees. Thus, let's study trees first:\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "A decision tree is a machine learning algorithm that works by *iteratively* and *greedily* split the decision space into regions. Each element to predict is then placed on one of the regions and predicted with the mean of the elements in the region (or majority class in it). To do so, it works like thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split():\n",
    "    # Check all the possible splits for a given var\n",
    "    # Return the split that minimizes the error. \n",
    "\n",
    "def find_one_split():\n",
    "    # Find for each var the best split possible (`find_best_split`)\n",
    "    # Use the one that yields the lowest error\n",
    "\n",
    "while (no_stopping_rule):\n",
    "    find_one_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the algorithm iteratively splits and splits until a stopping rule is given. That stopping rule can be:\n",
    "\n",
    "- The max_depth of the tree. i.e., how many splits to perform.\n",
    "- The min_num_in_leaf. i.e., the minimum number of observations to have at the end of the tree at each region. \n",
    "\n",
    "## Bagging \n",
    "\n",
    "Bagging is a machine learning technique for combining multiple *uncorrelated weak learners* and averaging over them to perform the final prediction. Weak learners because, by themselves, none of them performs particurly well: that is, all of them commit prediction errors. However, because some trees overpredict and others underpredict (that is, they commit different errors), when averaging the errors cancel each other and the resulting prediction is much better than the resulting from any individual tree. \n",
    "\n",
    "Thus, the way to create a forest using bagging is to ensure that the trees are uncorrelated. Random forest uses two techniques to decorrelate them:\n",
    "\n",
    "1. Bootstraped samples for each tree to work all the way down. \n",
    "2. Random subset of features at each split for each tree.\n",
    "\n",
    "### Bootstraped samples\n",
    "\n",
    "Given that each tree is possibly working with a very different sample from the dataset, the resulting predictions will be uncorrelated. Thus, as we average over this different trees, our resulting prediction will be better. \n",
    "\n",
    "#### Out of Bag (OOB) Samples\n",
    "\n",
    "Given the nature of our experiment, each observation won't be used (possibly) in some of the trees. Thus, using those trees, we could predict that observation and use it as a validation proxy. Note that because we did not use all the trees in the prediction (the trees that did use that validation in their training), **our prediction will not be as good as it could possibly be**. \n",
    "\n",
    "If we repeat this process for each observation in the dataset, we would have done it for what amounts as a validation set. Thus, this out of bag samples are a type of validation set. Note that because we are not using our best predictions to do so, ** this out of bag (OOB) error should underperform relative to a proper validation set error**. \n",
    "\n",
    "### Random subset of features\n",
    "\n",
    "We may use bootstraped samples, and still not get uncorrelated (different) predictions if, no matter what subsample each tree uses, the same features are always used. To avoid a very predictive set of features to dominate each weak learner, we can do the following: **at each possible split at each tree, consider only a random subset of the features to find the best possible split**. Thus, predictions at each tree will be ever more uncorrelated and thus the averaging will work even better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "def set_rf_samples(n):\n",
    "    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n",
    "    n random rows.\n",
    "    \"\"\"\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n))\n",
    "    \n",
    "def reset_rf_samples():\n",
    "    \"\"\" Undoes the changes produced by set_rf_samples.\n",
    "    \"\"\"\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n_samples))\n",
    "    \n",
    "def print_score(m):\n",
    "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
    "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify how many samples are passed to each tree, and thus speed up or decrease the training process and thus do it more iteratively. Even, to use random forest on large samples as we said on the introduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rf_samples(1000) # give each tree just 1000 samples\n",
    "reset_rf_samples() # Undoes the changes produced by set_rf_samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRegressor(n_estimators=40, min_samples_leaf=3, \n",
    "                      max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "# Hyperparameters to tune with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Driven Exploratory Data Analysis\n",
    "\n",
    "Jeremy says that you should get a Random Forest up and running as fast as possible, not only because it simply works, but because it will allow you to do **model-driven exploratory data analysis**. Given the algorithm's flexibility, you can thus do the following: \n",
    "\n",
    "- confidence on predictions. \n",
    "- feature importance. \n",
    "- partial dependence. \n",
    "- tree interpreter. \n",
    "- see which features extrapolate well to the validation/test set (using feature importance).\n",
    "\n",
    "Note that using the technique that Jeremy uses to compute feature importance (training and introducing noise in a variable and see how does the error change), we can also do the same for any machine learning algortihm. \n",
    "\n",
    "### Confidence on predictions\n",
    "\n",
    "Due to the nature of bagging, each observation is predicted using multiple single trees. To see how confident we are on each prediction, we can compute the standard deviation of these predictions.\n",
    "\n",
    "### Feature importance\n",
    "\n",
    "We can train the model as we regularly do and then do the following for each variable:\n",
    "\n",
    "1. Randomly shuffle the values for that variable across the dataset, thus making the variable pure noise.\n",
    "2. Predict this altered dataset and see how did the training error changed. \n",
    "3. Annotate this change in the training error. \n",
    "\n",
    "Thus, the most important features will be those that caused the worst change in the training error. \n",
    "\n",
    "Note: scikit-learn can use a different way (ensemble-tree specific) to calculate feature importance. \n",
    "\n",
    "### Partial dependence\n",
    "\n",
    "Sometimes we wonder, what would happen were we only change one variable and let all the others stay the same. We can train, change the all of the values for a given variable to a specific one and plot the result for each observation. Then do the same for all the unique values in the given variable and join the dots for each observation. Doing so, we have a partial plot dependence that allows us to examine what happens for each observation as we change the values of only one variable. \n",
    "\n",
    "### Tree interpreter\n",
    "\n",
    "Given a prediction, we can calculate how much of the change, relative to the dataset average, is due to each of the variables. To do so, we traverse each tree and check every split: annotate the variable that gave the split, and annotate the change in mean prediction relative to the last region. We do so for each tree and at the end we will have annotated how much the mean changed due to each of the variables used in the overall algorithm.\n",
    "\n",
    "### Extrapolation\n",
    "\n",
    "Extrapolation is a very cool trick when you have non-random validation/test sets. First thing to do is see if the validation is non-random (this is obvious when you are working with a date component) by predicting whether a variable is or is not in the validation/test set using all of your variables. If you get a good prediction, then the validation/test set is not random.\n",
    "\n",
    "You could do feature importance on this model and thus know which of the variables are the ones that change the most from the training to the validation/test set. Then, you could start dropping some of them as they are essentially noise and thus improve your overall fit on the validation set. That is, you focus on the variables that essentially work. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
